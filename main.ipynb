{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1c34396-f1eb-4977-87ed-a70ff8e61d11",
   "metadata": {},
   "source": [
    "# Lexicap\n",
    "\n",
    "This notebook aims to index [Lex Fridman's podcasts](https://www.youtube.com/playlist?list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4) \n",
    "transcriptions for Question-Answering using [Andrej Karathy's](https://twitter.com/karpathy) transcriptions produced with [OpenAI's whisper](https://github.com/openai/whisper/blob/main/model-card.md) üëâÔ∏è i.e.: [Lexicap](https://karpathy.ai/lexicap/).\n",
    "\n",
    "At the moment this code relies on a private package from MeliorAI, namely [distributed-faiss](https://github.com/MeliorAI/distributed-faiss) for distributed indexing using [FAISS](https://github.com/facebookresearch/faiss) as the underlying index. Every other package is openly available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35ef4ca-b2df-412e-911d-64a0ae90ad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip\n",
    "!pip install -qq \\\n",
    "    'distributed_faiss~=0.1.0' \\\n",
    "    nltk \\\n",
    "    tabulate \\\n",
    "    sentence_transformers \\\n",
    "    webvtt-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c495e775-69a7-4d2c-9db8-5d9f27c35d5f",
   "metadata": {},
   "source": [
    "## üìöÔ∏è Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e52d19e9-c55c-4b3b-9672-08e883e3aa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "            \n",
    "\n",
    "DATA_DIR = \"./data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5be80cc-d4bd-46cc-b815-82379fc0b492",
   "metadata": {},
   "source": [
    "## üîß Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f72232-986c-40ec-8f6a-7282806ef802",
   "metadata": {},
   "source": [
    "### üéôÔ∏è VTT utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2af8288-9cc6-46fc-87ba-3daf408bd75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import webvtt\n",
    "\n",
    "from tabulate import tabulate\n",
    "from tqdm.auto import tqdm\n",
    "from typing import *\n",
    "\n",
    "\n",
    "def gather_transcripts(data_dir:str = DATA_DIR, load_small:bool = False):\n",
    "    mask = \"*_small.vtt\" if load_small else \"*_large.vtt\"\n",
    "    return sorted(\n",
    "        glob.glob(os.path.join(data_dir, \"vtt\", mask))\n",
    "    )\n",
    "\n",
    "\n",
    "def gather_episode_data(data_dir:str = DATA_DIR):\n",
    "    ep_data = {}\n",
    "    with open(os.path.join(data_dir, \"episode_names.txt\")) as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            ep_num = re.findall(\"(?<=#)\\d+\", line)[0]\n",
    "            line = re.sub(f\"^{ep_num} \", \"\", line)\n",
    "            guest_and_title = line.split(\" | \")[0]\n",
    "            ep_data[ep_num] = {\n",
    "                \"guest\": guest_and_title.split(\": \")[0], \n",
    "                \"title\": \": \".join(guest_and_title.split(\": \")[1:])\n",
    "            }\n",
    "    \n",
    "    return ep_data\n",
    "\n",
    "    \n",
    "def sent_split(text:str) -> Dict[str, Union[str, int]]:\n",
    "    \"\"\"Divides each document section into several chunks based on\n",
    "    a NLTK **sentence** tokenizer.\n",
    "    \"\"\"\n",
    "    text_chunks = nltk.tokenize.sent_tokenize(text)\n",
    "    text_indices = list(map(\n",
    "        lambda m: (m.start(), m.end()) if m else (None, None),\n",
    "        [re.search(re.escape(text_chunk), text) for text_chunk in text_chunks]\n",
    "    ))\n",
    "    \n",
    "    return [\n",
    "        {\"text\": text, \"start\": indices[0], \"end\": indices[1]} \n",
    "        for text, indices in zip(text_chunks, text_indices)\n",
    "    ]\n",
    "\n",
    "\n",
    "def print_datapoint(datapoint, lim:int = -1):\n",
    "    keys = [\"ep_num\", \"guest\", \"title\"]\n",
    "    print(tabulate([[datapoint[k] for k in keys]], \n",
    "                   headers=keys,\n",
    "                  tablefmt=\"grid\"))\n",
    "    \n",
    "    readout_list = list(map(lambda x: \n",
    "                            (x['text'], str(x['start']), str(x['end'])), \n",
    "                            datapoint[\"text_chunks\"][:lim])\n",
    "                       )\n",
    "    print(tabulate(readout_list, \n",
    "                   headers=[\"text\", \"start\", \"end\"],\n",
    "                   tablefmt=\"grid\",\n",
    "                   maxcolwidths=55))\n",
    "    \n",
    "    \n",
    "def build_dataset(split_by_time:bool = False):\n",
    "    \n",
    "    def split_with_timestamp(sfile:str):\n",
    "        chunks = []\n",
    "        buffer = []\n",
    "        start = None\n",
    "        for caption in webvtt.read(sfile):\n",
    "            text = caption.text\n",
    "            if not re.search(\"\\w[\\.\\?\\!]$$\", text):\n",
    "                buffer.append(text)\n",
    "                if start is None:\n",
    "                    start = caption.start\n",
    "            else:\n",
    "                chunks.append({\n",
    "                    \"start\": start or caption.start,\n",
    "                    \"end\": caption.end,\n",
    "                    \"text\": (\" \".join(buffer) + text).strip()\n",
    "                })\n",
    "                buffer = []\n",
    "                start = None\n",
    "\n",
    "        return chunks\n",
    "    \n",
    "    def split_with_text_indices(sfile:str):\n",
    "        text = \" \".join([caption.text for caption in webvtt.read(sfile)])\n",
    "        return sent_split(text)\n",
    "        \n",
    "        \n",
    "    script_files = gather_transcripts()\n",
    "    ep_data = gather_episode_data()\n",
    "    print(f\"Total episodes transcripts: {len(script_files)}\")\n",
    "\n",
    "    # Compile a dataset of transcripts with its episode information\n",
    "    dataset = []\n",
    "    data_iter = tqdm(script_files)\n",
    "    sent_id = 0\n",
    "    for sfile in data_iter:\n",
    "        fname = os.path.basename(sfile)\n",
    "        data_iter.set_description(fname)\n",
    "\n",
    "        # Episode number\n",
    "        ep_num = str(int(re.findall(\"\\d{1,3}\", fname)[0]))  # trim leading 0's\n",
    "        ep_info = ep_data[ep_num]\n",
    "\n",
    "        # Read transcript and split in sentences\n",
    "        if split_by_time:\n",
    "            chunks = split_with_timestamp(sfile)\n",
    "        else:\n",
    "            chunks = split_with_text_indices(sfile)\n",
    "            \n",
    "        nt = len(chunks)\n",
    "\n",
    "        ep_info.update({\n",
    "            \"text_ids\": list(range(sent_id, sent_id + nt)),\n",
    "            \"text_chunks\": chunks,\n",
    "            \"ep_num\": ep_num\n",
    "        })\n",
    "        \n",
    "        dataset.append(ep_info)\n",
    "        sent_id += nt\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e265783-f15d-4fce-aeed-32970a9fd224",
   "metadata": {},
   "source": [
    "### ‚è±Ô∏è Timestamp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "363ac7a1-0a71-4c5e-88b3-3e28814710f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "def cluster_in_time(datapoint, group_secs:int = 60):\n",
    "    \n",
    "    def parse_ts(ts:str):\n",
    "        return dt.datetime.strptime(ts, \"%H:%M:%S.%f\")\n",
    "\n",
    "\n",
    "    def delta_secs(t1:str, t2:str):\n",
    "        return (parse_ts(t2) - parse_ts(t1)).seconds\n",
    "    \n",
    "    def add_group():\n",
    "        groups.append({\n",
    "            \"start\": start['start'],\n",
    "            \"end\": end['end'],\n",
    "            \"text\": \" \".join([b['text'] for b in buffer])\n",
    "        })\n",
    "        \n",
    "    chunks = datapoint['text_chunks']\n",
    "    groups = []\n",
    "    ini = 0\n",
    "    fin = 1\n",
    "    start = chunks[ini]\n",
    "    end = chunks[fin]\n",
    "    buffer = [start]\n",
    "    while fin < len(chunks):\n",
    "        end = chunks[fin]\n",
    "        \n",
    "        if delta_secs(t1=start['start'], t2=end['end']) < group_secs:\n",
    "            buffer.append(end)\n",
    "            \n",
    "        else:\n",
    "            add_group()\n",
    "            ini = fin\n",
    "            start = chunks[ini]\n",
    "            buffer = [end]\n",
    "            \n",
    "        fin += 1\n",
    "        \n",
    "    if len(buffer) > 0:\n",
    "        add_group()\n",
    "        \n",
    "    return groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a50669f-73ce-48fe-8da9-292b498f5431",
   "metadata": {},
   "source": [
    "## üëÄ Dataset visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1ad73605-e4eb-4e9f-9293-97450bdc3b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total episodes transcripts: 319\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b52b177ec764427a1709e2733ed303c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+\n",
      "|   ep_num | guest       | title    |\n",
      "+==========+=============+==========+\n",
      "|        1 | Max Tegmark | Life 3.0 |\n",
      "+----------+-------------+----------+\n",
      "+--------------------------------------------------------+--------------+--------------+\n",
      "| text                                                   | start        | end          |\n",
      "+========================================================+==============+==============+\n",
      "| As part of MIT course 6S099, Artificial General        | 00:00:00.000 | 00:00:06.600 |\n",
      "| Intelligence, I've gotten the chance to sit down with  |              |              |\n",
      "| Max Tegmark.                                           |              |              |\n",
      "+--------------------------------------------------------+--------------+--------------+\n",
      "| He is a professor here at MIT.                         | 00:00:06.600 | 00:00:08.680 |\n",
      "+--------------------------------------------------------+--------------+--------------+\n",
      "| He's a physicist, spent a large part of his career     | 00:00:08.680 | 00:00:16.960 |\n",
      "| studying the mysteries of our cosmological universe.   |              |              |\n",
      "+--------------------------------------------------------+--------------+--------------+\n",
      "| But he's also studied and delved into the beneficial   | 00:00:16.960 | 00:00:25.800 |\n",
      "| possibilities and the existential risks of artificial  |              |              |\n",
      "| intelligence.                                          |              |              |\n",
      "+--------------------------------------------------------+--------------+--------------+\n",
      "| Amongst many other things, he is the cofounder  of the | 00:00:25.800 | 00:00:35.160 |\n",
      "| Future of Life Institute, author of two books, both of |              |              |\n",
      "| which I highly recommend.                              |              |              |\n",
      "+--------------------------------------------------------+--------------+--------------+\n",
      "+----------+-------------+----------+\n",
      "|   ep_num | guest       | title    |\n",
      "+==========+=============+==========+\n",
      "|        1 | Max Tegmark | Life 3.0 |\n",
      "+----------+-------------+----------+\n",
      "+---------------------------------------------------------+--------------+--------------+\n",
      "| text                                                    | start        | end          |\n",
      "+=========================================================+==============+==============+\n",
      "| As part of MIT course 6S099, Artificial General         | 00:00:00.000 | 00:01:01.080 |\n",
      "| Intelligence, I've gotten the chance to sit down with   |              |              |\n",
      "| Max Tegmark. He is a professor here at MIT. He's a      |              |              |\n",
      "| physicist, spent a large part of his career studying    |              |              |\n",
      "| the mysteries of our cosmological universe. But he's    |              |              |\n",
      "| also studied and delved into the beneficial             |              |              |\n",
      "| possibilities and the existential risks of artificial   |              |              |\n",
      "| intelligence. Amongst many other things, he is the      |              |              |\n",
      "| cofounder  of the Future of Life Institute, author of   |              |              |\n",
      "| two books, both of which I highly recommend. First, Our |              |              |\n",
      "| Mathematical Universe. Second is Life 3.0. He's truly   |              |              |\n",
      "| an out of the box thinker and a fun personality, so I   |              |              |\n",
      "| really enjoy talking to him. If you'd like to see more  |              |              |\n",
      "| of these videos in the future,  please subscribe and    |              |              |\n",
      "| also click the little bell icon to make sure you don't  |              |              |\n",
      "| miss any videos.                                        |              |              |\n",
      "+---------------------------------------------------------+--------------+--------------+\n",
      "| Also, Twitter, LinkedIn, agi.mit.edu  if you wanna      | 00:00:52.720 | 00:02:00.200 |\n",
      "| watch other lectures or conversations like this one.    |              |              |\n",
      "| Better yet, go read Max's book, Life 3.0. Chapter seven |              |              |\n",
      "| on goals is my favorite. It's really where philosophy   |              |              |\n",
      "| and engineering come together and it opens with a quote |              |              |\n",
      "| by Dostoevsky. The mystery of human existence lies not  |              |              |\n",
      "| in just staying alive but in finding something to live  |              |              |\n",
      "| for. Lastly, I believe that every failure rewards us    |              |              |\n",
      "| with an opportunity to learn  and in that sense, I've   |              |              |\n",
      "| been very fortunate  to fail in so many new and         |              |              |\n",
      "| exciting ways and this conversation was no different.   |              |              |\n",
      "| I've learned about something called radio frequency     |              |              |\n",
      "| interference, RFI, look it up. Apparently, music and    |              |              |\n",
      "| conversations  from local radio stations can bleed into |              |              |\n",
      "| the audio  that you're recording in such a way that it  |              |              |\n",
      "| almost completely ruins that audio. It's an             |              |              |\n",
      "| exceptionally difficult sound source to remove.         |              |              |\n",
      "+---------------------------------------------------------+--------------+--------------+\n",
      "| So, I've gotten the opportunity to learn how to avoid   | 00:01:53.240 | 00:02:54.560 |\n",
      "| RFI in the future during recording sessions. I've also  |              |              |\n",
      "| gotten the opportunity to learn  how to use Adobe       |              |              |\n",
      "| Audition and iZotope RX 6 to do some noise, some audio  |              |              |\n",
      "| repair. Of course, this is an exceptionally difficult   |              |              |\n",
      "| noise to remove. I am an engineer. I'm not an audio     |              |              |\n",
      "| engineer. Neither is anybody else in our group but we   |              |              |\n",
      "| did our best. Nevertheless, I thank you for your        |              |              |\n",
      "| patience and I hope you're still able to enjoy this     |              |              |\n",
      "| conversation. Do you think there's intelligent life out |              |              |\n",
      "| there in the universe? Let's open up with an easy       |              |              |\n",
      "| question. I have a minority view here actually. When I  |              |              |\n",
      "| give public lectures, I often ask for a show of hands   |              |              |\n",
      "| who thinks there's intelligent life out there somewhere |              |              |\n",
      "| else  and almost everyone put their hands up  and when  |              |              |\n",
      "| I ask why, they'll be like, oh, there's so many         |              |              |\n",
      "| galaxies out there, there's gotta be.                   |              |              |\n",
      "+---------------------------------------------------------+--------------+--------------+\n",
      "| But I'm a numbers nerd, right? So when you look more    | 00:02:51.840 | 00:03:56.280 |\n",
      "| carefully at it, it's not so clear at all. When we talk |              |              |\n",
      "| about our universe, first of all, we don't mean all of  |              |              |\n",
      "| space. We actually mean, I don't know,  you can throw   |              |              |\n",
      "| me the universe if you want, it's behind you there.     |              |              |\n",
      "| It's, we simply mean the spherical region of space      |              |              |\n",
      "| from which light has a time to reach us so far  during  |              |              |\n",
      "| the 14.8 billion year, 13.8 billion years since our Big |              |              |\n",
      "| Bang. There's more space here but this is what we call  |              |              |\n",
      "| a universe because that's all we have access to. So is  |              |              |\n",
      "| there intelligent life here  that's gotten to the point |              |              |\n",
      "| of building telescopes and computers? My guess is no,   |              |              |\n",
      "| actually. The probability of it happening on any given  |              |              |\n",
      "| planet is some number we don't know what it is.         |              |              |\n",
      "+---------------------------------------------------------+--------------+--------------+\n",
      "| And what we do know is that the number can't be super   | 00:03:42.620 | 00:04:45.960 |\n",
      "| high  because there's over a billion Earth like planets |              |              |\n",
      "| in the Milky Way galaxy alone, many of which are        |              |              |\n",
      "| billions of years older than Earth. And aside from some |              |              |\n",
      "| UFO believers,  there isn't much evidence that any      |              |              |\n",
      "| superduran civilization has come here at all. And so    |              |              |\n",
      "| that's the famous Fermi paradox, right? And then if you |              |              |\n",
      "| work the numbers,  what you find is that if you have no |              |              |\n",
      "| clue  what the probability is of getting life on a      |              |              |\n",
      "| given planet,  so it could be 10 to the minus 10, 10 to |              |              |\n",
      "| the minus 20,  or 10 to the minus two, or any power of  |              |              |\n",
      "| 10  is sort of equally likely  if you wanna be really   |              |              |\n",
      "| open minded,  that translates into it being equally     |              |              |\n",
      "| likely  that our nearest neighbor is 10 to the 16       |              |              |\n",
      "| meters away, 10 to the 17 meters away, 10 to the 18.    |              |              |\n",
      "+---------------------------------------------------------+--------------+--------------+\n",
      "| By the time you get much less than 10 to the 16         | 00:04:35.400 | 00:05:43.720 |\n",
      "| already, we pretty much know there is nothing else that |              |              |\n",
      "| close. And when you get beyond 10. Because they would   |              |              |\n",
      "| have discovered us. Yeah, they would have been          |              |              |\n",
      "| discovered as long ago,  or if they're really close,    |              |              |\n",
      "| we would have probably noted some engineering projects  |              |              |\n",
      "| that they're doing. And if it's beyond 10 to the 26     |              |              |\n",
      "| meters, that's already outside of here. So my guess is  |              |              |\n",
      "| actually that we are the only life in here  that's      |              |              |\n",
      "| gotten the point of building advanced tech,  which I    |              |              |\n",
      "| think is very, puts a lot of responsibility on our      |              |              |\n",
      "| shoulders, not screw up. I think people who take for    |              |              |\n",
      "| granted  that it's okay for us to screw up,  have an    |              |              |\n",
      "| accidental nuclear war or go extinct somehow  because   |              |              |\n",
      "| there's a sort of Star Trek like situation out there    |              |              |\n",
      "| where some other life forms are gonna come and bail us  |              |              |\n",
      "| out and it doesn't matter as much. I think they're      |              |              |\n",
      "| leveling us into a false sense of security.             |              |              |\n",
      "+---------------------------------------------------------+--------------+--------------+\n",
      "| I think it's much more prudent to say,  let's be really | 00:05:33.400 | 00:06:33.520 |\n",
      "| grateful  for this amazing opportunity we've had and    |              |              |\n",
      "| make the best of it just in case it is down to us. So   |              |              |\n",
      "| from a physics perspective,  do you think intelligent   |              |              |\n",
      "| life,  so it's unique from a sort of statistical view   |              |              |\n",
      "| of the size of the universe,  but from the basic matter |              |              |\n",
      "| of the universe, how difficult is it for intelligent    |              |              |\n",
      "| life to come about? The kind of advanced tech building  |              |              |\n",
      "| life  is implied in your statement that it's really     |              |              |\n",
      "| difficult to create something like a human species.     |              |              |\n",
      "| Well, I think what we know is that going from no life   |              |              |\n",
      "| to having life that can do a level of tech,  there's    |              |              |\n",
      "| some sort of two going beyond that than actually        |              |              |\n",
      "| settling our whole universe with life.                  |              |              |\n",
      "+---------------------------------------------------------+--------------+--------------+\n",
      "| There's some major roadblock there,  which is some      | 00:06:22.200 | 00:07:30.000 |\n",
      "| great filter as it's sometimes called, which is tough   |              |              |\n",
      "| to get through. It's either that roadblock is either    |              |              |\n",
      "| behind us or in front of us. I'm hoping very much that  |              |              |\n",
      "| it's behind us. I'm super excited every time we get a   |              |              |\n",
      "| new report from NASA saying they failed to find any     |              |              |\n",
      "| life on Mars. I'm like, yes, awesome. Because that      |              |              |\n",
      "| suggests that the hard part,  maybe it was getting the  |              |              |\n",
      "| first ribosome  or some very low level kind of stepping |              |              |\n",
      "| stone so that we're home free. Because if that's true,  |              |              |\n",
      "| then the future is really only limited by our own       |              |              |\n",
      "| imagination. It would be much suckier if it turns out   |              |              |\n",
      "| that this level of life is kind of a dime a dozen, but  |              |              |\n",
      "| maybe there's some other problem. Like as soon as a     |              |              |\n",
      "| civilization gets advanced technology,  within a        |              |              |\n",
      "| hundred years, they get into some stupid fight with     |              |              |\n",
      "| themselves and poof. That would be a bummer.            |              |              |\n",
      "+---------------------------------------------------------+--------------+--------------+\n",
      "| Yeah, so you've explored the mysteries of the universe, | 00:07:21.760 | 00:08:25.120 |\n",
      "| the cosmological universe, the one that's sitting       |              |              |\n",
      "| between us today. I think you've also begun to explore  |              |              |\n",
      "| the other universe,  which is sort of the mystery,  the |              |              |\n",
      "| mysterious universe of the mind of intelligence, of     |              |              |\n",
      "| intelligent life. So is there a common thread between   |              |              |\n",
      "| your interest or the way you think about space and      |              |              |\n",
      "| intelligence? Oh yeah, when I was a teenager, I was     |              |              |\n",
      "| already very fascinated by the biggest questions. And I |              |              |\n",
      "| felt that the two biggest mysteries of all in science   |              |              |\n",
      "| were our universe out there and our universe in here.   |              |              |\n",
      "| So it's quite natural after having spent  a quarter of  |              |              |\n",
      "| a century on my career,  thinking a lot about this one, |              |              |\n",
      "| that I'm now indulging in the luxury of doing research  |              |              |\n",
      "| on this one. It's just so cool.                         |              |              |\n",
      "+---------------------------------------------------------+--------------+--------------+\n",
      "| I feel the time is ripe now for you trans greatly       | 00:08:17.720 | 00:09:21.680 |\n",
      "| deepening our understanding of this. Just start         |              |              |\n",
      "| exploring this one. Yeah, because I think a lot of      |              |              |\n",
      "| people view intelligence  as something mysterious that  |              |              |\n",
      "| can only exist  in biological organisms like us,  and   |              |              |\n",
      "| therefore dismiss all talk about artificial general     |              |              |\n",
      "| intelligence as science fiction. But from my            |              |              |\n",
      "| perspective as a physicist,  I am a blob of quarks and  |              |              |\n",
      "| electrons  moving around in a certain pattern and       |              |              |\n",
      "| processing information in certain ways. And this is     |              |              |\n",
      "| also a blob of quarks and electrons. I'm not smarter    |              |              |\n",
      "| than the water bottle because I'm made of different     |              |              |\n",
      "| kinds of quarks. I'm made of up quarks and down quarks, |              |              |\n",
      "| exact same kind as this. There's no secret sauce, I     |              |              |\n",
      "| think, in me. It's all about the pattern of the         |              |              |\n",
      "| information processing.                                 |              |              |\n",
      "+---------------------------------------------------------+--------------+--------------+\n",
      "| And this means that there's no law of physics  saying   | 00:09:08.560 | 00:10:09.560 |\n",
      "| that we can't create technology,  which can help us by  |              |              |\n",
      "| being incredibly intelligent and help us crack          |              |              |\n",
      "| mysteries that we couldn't. In other words, I think     |              |              |\n",
      "| we've really only seen the tip of the intelligence      |              |              |\n",
      "| iceberg so far. Yeah, so the perceptronium. Yeah. So    |              |              |\n",
      "| you coined this amazing term. It's a hypothetical state |              |              |\n",
      "| of matter,  sort of thinking from a physics             |              |              |\n",
      "| perspective,  what is the kind of matter that can help, |              |              |\n",
      "| as you're saying, subjective experience emerge,         |              |              |\n",
      "| consciousness emerge. So how do you think about         |              |              |\n",
      "| consciousness from this physics perspective? Very good  |              |              |\n",
      "| question. So again, I think many people have            |              |              |\n",
      "| underestimated  our ability to make progress on this    |              |              |\n",
      "| by convincing themselves it's hopeless because somehow  |              |              |\n",
      "| we're missing some ingredient that we need.             |              |              |\n",
      "+---------------------------------------------------------+--------------+--------------+\n",
      "| There's some new consciousness particle or whatever. I  | 00:10:05.840 | 00:11:08.760 |\n",
      "| happen to think that we're not missing anything  and    |              |              |\n",
      "| that it's not the interesting thing  about              |              |              |\n",
      "| consciousness that gives us  this amazing subjective    |              |              |\n",
      "| experience of colors and sounds and emotions. It's      |              |              |\n",
      "| rather something at the higher level about the patterns |              |              |\n",
      "| of information processing. And that's why I like to     |              |              |\n",
      "| think about this idea of perceptronium. What does it    |              |              |\n",
      "| mean for an arbitrary physical system  to be conscious  |              |              |\n",
      "| in terms of what its particles are doing or its         |              |              |\n",
      "| information is doing? I don't think, I hate carbon      |              |              |\n",
      "| chauvinism,  this attitude you have to be made of       |              |              |\n",
      "| carbon atoms to be smart or conscious. There's          |              |              |\n",
      "| something about the information processing that this    |              |              |\n",
      "| kind of matter performs. Yeah, and you can see I have   |              |              |\n",
      "| my favorite equations here describing various           |              |              |\n",
      "| fundamental aspects of the world.                       |              |              |\n",
      "+---------------------------------------------------------+--------------+--------------+\n",
      "| I feel that I think one day,  maybe someone who's       | 00:11:00.720 | 00:12:01.000 |\n",
      "| watching this will come up  with the equations that     |              |              |\n",
      "| information processing has to satisfy to be conscious.  |              |              |\n",
      "| I'm quite convinced there is big discovery  to be made  |              |              |\n",
      "| there because let's face it, we know that so many       |              |              |\n",
      "| things are made up of information. We know that some    |              |              |\n",
      "| information processing is conscious because we are      |              |              |\n",
      "| conscious. But we also know that a lot of information   |              |              |\n",
      "| processing is not conscious. Like most of the           |              |              |\n",
      "| information processing happening in your brain right    |              |              |\n",
      "| now is not conscious. There are like 10 megabytes per   |              |              |\n",
      "| second coming in even just through your visual system.  |              |              |\n",
      "| You're not conscious about your heartbeat regulation or |              |              |\n",
      "| most things. Even if I just ask you to like read what   |              |              |\n",
      "| it says here, you look at it and then, oh, now you know |              |              |\n",
      "| what it said. But you're not aware of how the           |              |              |\n",
      "| computation actually happened. Your consciousness is    |              |              |\n",
      "| like the CEO that got an email at the end with the      |              |              |\n",
      "| final answer.                                           |              |              |\n",
      "+---------------------------------------------------------+--------------+--------------+\n",
      "| So what is it that makes a difference? I think that's   | 00:11:56.680 | 00:13:02.120 |\n",
      "| both a great science mystery. We're actually studying   |              |              |\n",
      "| it a little bit in my lab here  at MIT, but I also      |              |              |\n",
      "| think it's just a really urgent question to answer. For |              |              |\n",
      "| starters, I mean, if you're an emergency room doctor    |              |              |\n",
      "| and you have an unresponsive patient coming in,         |              |              |\n",
      "| wouldn't it be great if in addition to having  a CT     |              |              |\n",
      "| scanner, you had a consciousness scanner  that could    |              |              |\n",
      "| figure out whether this person  is actually having      |              |              |\n",
      "| locked in syndrome or is actually comatose. And in the  |              |              |\n",
      "| future, imagine if we build robots  or the machine that |              |              |\n",
      "| we can have really good conversations with, which I     |              |              |\n",
      "| think is very likely to happen. Wouldn't you want to    |              |              |\n",
      "| know if your home helper robot  is actually             |              |              |\n",
      "| experiencing anything or just like a zombie, I mean,    |              |              |\n",
      "| would you prefer it? What would you prefer?             |              |              |\n",
      "+---------------------------------------------------------+--------------+--------------+\n",
      "| Would you prefer that it's actually unconscious  so     | 00:12:54.360 | 00:13:56.840 |\n",
      "| that you don't have to feel guilty about switching it   |              |              |\n",
      "| off or giving boring chores or what would you prefer?   |              |              |\n",
      "| Well, certainly we would prefer, I would prefer the     |              |              |\n",
      "| appearance of consciousness. But the question is        |              |              |\n",
      "| whether the appearance of consciousness is different    |              |              |\n",
      "| than consciousness itself. And sort of to ask that as a |              |              |\n",
      "| question,  do you think we need to understand what      |              |              |\n",
      "| consciousness is,  solve the hard problem of            |              |              |\n",
      "| consciousness in order to build something like an AGI   |              |              |\n",
      "| system? No, I don't think that. And I think we will     |              |              |\n",
      "| probably be able to build things even if we don't       |              |              |\n",
      "| answer that question. But if we want to make sure that  |              |              |\n",
      "| what happens is a good thing, we better solve it first. |              |              |\n",
      "| So it's a wonderful controversy you're raising there    |              |              |\n",
      "| where you have basically three points of view about the |              |              |\n",
      "| hard problem. So there are two different points of      |              |              |\n",
      "| view.                                                   |              |              |\n",
      "+---------------------------------------------------------+--------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "dataset = build_dataset(split_by_time=True)\n",
    "\n",
    "# print one of the elements of the dataset\n",
    "print_datapoint(dataset[0], lim=5)\n",
    "\n",
    "# print the same data point with text grouped in ~60 second chunks\n",
    "dat = copy.deepcopy(dataset[0])\n",
    "dat.update({\"text_chunks\": cluster_in_time(dat)})\n",
    "print_datapoint(dat, lim=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637cbfa6-bb6c-4312-963a-d10e936157df",
   "metadata": {},
   "source": [
    "## üß™ Semantic Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90232e74-26e1-4c33-a894-a2ff1e3ddd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Can also be used from ü§ó-Transformers:\n",
    "# https://huggingface.co/sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco\n",
    "embedder = SentenceTransformer('msmarco-distilbert-base-tas-b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7db6c5c3-b639-4955-a770-5d73879708c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c034b1b66f4343b151dc24c4434b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_iter = tqdm(dataset)\n",
    "\n",
    "for dpoint in data_iter:\n",
    "    data_iter.set_description(f\"{dpoint['ep_num']}: {dpoint['title']}\")\n",
    "    dpoint[\"embeddings\"] = embedder.encode(dpoint['texts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3fc44ee-2a68-4043-8572-af4ce3b730b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(737, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"embeddings\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077fd007-9438-47e0-8b0d-f25bd5fdbf56",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üì¶Ô∏è Indexing: Distributed FAISS\n",
    "\n",
    "First we need to run the FAISS server (in a separate terminal):\n",
    "\n",
    "```python\n",
    "python dfaiss_server.py \\\n",
    "    --log-dir ./logs \\\n",
    "    --partition 1 \\\n",
    "    --discovery-config dfaiss.discovery \\\n",
    "    --num-servers 1 \\\n",
    "    --num-servers-per-node 1 \\\n",
    "    --mem-gb 4 \\\n",
    "    --load-index\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bef92811-60c2-47ae-bee7-10dfb320f606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "from distributed_faiss.client import IndexClient\n",
    "from distributed_faiss.index_cfg import IndexCfg\n",
    "\n",
    "\n",
    "def _validate_metric(self):\n",
    "    if self.metric not in VALID_METRICS:\n",
    "        logger.error(\n",
    "            f\"{self.metric} is not a valid Metric. \"\n",
    "            f\"Try to choose between {VALID_METRICS}\"\n",
    "        )\n",
    "        raise ValueError(\n",
    "            f\"{self.metric} is not a valid Metric. \"\n",
    "            f\"Try to choose between {VALID_METRICS}\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "def init_client(index_id:str, index_cfg_file:str, discovery_file:str):\n",
    "    with open(index_cfg_file, \"r\") as f:\n",
    "        cfg = json.load(f)\n",
    "        idx_cfg = IndexCfg(**cfg)\n",
    "\n",
    "    index_client = IndexClient(discovery_file)\n",
    "\n",
    "    if not index_client.index_loaded(index_id):\n",
    "        idx_loaded = index_client.load_index(index_id, idx_cfg)\n",
    "        if idx_loaded is False:\n",
    "            print(f\"Index {index_id} hasn't been loaded. Creating new index...\")\n",
    "            index_client.create_index(index_id, idx_cfg)\n",
    "            \n",
    "    return index_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7600bf7-b522-4054-9054-650ce241d8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_INDEX_TYPES = [\"Flat\", \"IVF\"]\n",
    "VALID_METRICS = [\n",
    "    \"l2\",\n",
    "    \"dot\",\n",
    "]\n",
    "\n",
    "# configuration and discovery files\n",
    "discovery_file = \"./dfaiss.discovery\"\n",
    "index_cfg_file = \"./idx_cfg.json\"\n",
    "index_id = \"local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5792299-0d07-4573-8da0-bc8ec7c76caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting jose-N501VW 12032 AddressFamily.AF_INET\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca1427e20c544d8b16f932bfe1b4032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the dfaiss client\n",
    "index_client = init_client(index_id, index_cfg_file, discovery_file)\n",
    "\n",
    "data_iter = tqdm(dataset)\n",
    "\n",
    "for dpoint in data_iter:\n",
    "    data_iter.set_description(f\"{dpoint['ep_num']}: {dpoint['title']}\")\n",
    "    embeddings = dpoint[\"embeddings\"]\n",
    "    n_vec = embeddings.shape[0]\n",
    "    meta = {k: dpoint[k] for k in [\"ep_num\", \"guest\", \"title\"]}\n",
    "    index_client.add_index_data(\n",
    "        index_id, \n",
    "        embeddings, \n",
    "        [{\"id\": tid, **meta} for tid in dpoint[\"text_ids\"]]\n",
    "    )\n",
    "    \n",
    "index_client.sync_train(index_id)\n",
    "index_client.add_buffer_to_index(index_id)\n",
    "index_client.save_index(index_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "11c8e720-ecb1-4ddc-924f-0c5b0e2faf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting jose-N501VW 12032 AddressFamily.AF_INET\n",
      "connecting jose-N501VW 12033 AddressFamily.AF_INET\n"
     ]
    }
   ],
   "source": [
    "index_client = init_client(index_id, index_cfg_file, discovery_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af04c6f1-b2a5-4484-8a0a-a75e2979bdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num servers: 1\n",
      "Server 1: IndexState.ADD\n"
     ]
    }
   ],
   "source": [
    "print(f\"num servers: {index_client.get_num_servers()}\")\n",
    "for sn, state in enumerate(index_client.get_all_states(index_id)):\n",
    "    print(f\"Server {sn+1}: {state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7da2688-6195-4b08-b101-eb64a691c7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76984"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_client.get_ntotal(index_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5714f8a2-9de7-4b8c-a2d3-c4d4893e88e0",
   "metadata": {},
   "source": [
    "## üîçÔ∏è QA Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7c7ec8a-a285-460f-b4c0-bbb619cc3e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index loaded: True\n",
      "[[{'id': 96511}, {'id': 25577}, {'id': 96600}, {'id': 56700}, {'id': 64434}, {'id': 0}, {'id': 5391}, {'id': 1808}, {'id': 149383}, {'id': 31468}]]\n",
      "[array([-99.35968 , -98.620026, -97.52474 , -97.056656, -96.73923 ,\n",
      "       -96.72657 , -95.80153 , -95.267555, -93.477715, -93.45641 ],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "index_loaded = any(index_client.all_index_loaded(index_id))\n",
    "if not index_loaded:\n",
    "    index_client.load_index(index_id)\n",
    "    \n",
    "index_loaded = any(index_client.all_index_loaded(index_id))\n",
    "print(f\"Index loaded: {index_loaded}\")\n",
    "\n",
    "scores, indices = index_client.search(\n",
    "    embedder.encode([\"Which boooks where written by Max Tegmark?\"]),\n",
    "    top_k=10,\n",
    "    index_id=index_id,\n",
    "    return_embeddings=False,\n",
    "    filter_dict={},\n",
    ")\n",
    "print(indices)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a39f92ea-a7ab-4dd9-b144-8d9150bac91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_to_dp_idx = {\n",
    "    dpoint[\"ep_num\"]: i \n",
    "    for i, dpoint in enumerate(dataset)\n",
    "}\n",
    "\n",
    "tid_to_ep = {\n",
    "    tid: dpoint[\"ep_num\"]\n",
    "    for dpoint in dataset\n",
    "    for tid in dpoint[\"text_ids\"]\n",
    "}\n",
    "\n",
    "\n",
    "def tid_to_episode(tid:int):\n",
    "    return dataset[ep_to_dp_idx[tid_to_ep[tid]]]\n",
    "\n",
    "\n",
    "def tid_to_text(tid:int):\n",
    "    epi = tid_to_episode(tid)\n",
    "    idx = epi[\"text_ids\"].index(tid)\n",
    "    return epi[\"texts\"][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b402d042-5b15-455d-86e9-162411833566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96511 --> ('And I guess I follow Max Tegmark here.', (154099, 154137))\n",
      "25577 --> (\"And by the way, I'm borrowing from Max Tegmark  for some of these metaphors, the physicist.\", (93069, 93160))\n",
      "96600 --> (\"I mean, talking about people like Lee Small  and Alan Guth, Max Tegmark, okay, we're really smart.\", (163510, 163608))\n",
      "56700 --> (\"Tegmark, I view as a philosopher  who is somehow taking credit for Platonism,  which I don't see any reason for fighting with Max  because I like Max, but if it ever comes time,  I'm putting a post it note that I'm not positive  the mathematical universe hypothesis  is really anything new.\", (125870, 126160))\n",
      "64434 --> ('Like you have like the Max Tegmark,  young version of Max Tegmark,  who knows how to play the role of boring and fitting in.', (80770, 80894))\n",
      "0 --> (\" As part of MIT course 6S099, Artificial General Intelligence,  I've gotten the chance to sit down with Max Tegmark.\", (0, 116))\n",
      "5391 --> ('Actually, a lot of physicists, Max Tegmark,  people who think the universe  is an information processing system,  our brain is kind of an information processing system.', (3340, 3508))\n",
      "1808 --> ('So I talked to Max Tegmark and Stuart Russell,  who are very concerned about existential threats of AI.', (51379, 51482))\n",
      "149383 --> ('And then Allen Ginsberg wrote a poem in the 60s,  which incredible poem called Howl about this thing Moloch.', (75828, 75936))\n",
      "31468 --> (\"Well, he'd be on like TikTok and Instagram  and he would never write the great works he's written.\", (27435, 27533))\n"
     ]
    }
   ],
   "source": [
    "for ind in indices[0]:\n",
    "    tid = ind[\"id\"]\n",
    "    print(f\"{tid} --> {tid_to_text(tid)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce3ce89d961123fe0e9a7a924b943e5a68f78d947d2f271381be2ef8c6d8fc3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
