{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1c34396-f1eb-4977-87ed-a70ff8e61d11",
   "metadata": {},
   "source": [
    "# Lexicap\n",
    "\n",
    "This notebook aims to index [Lex Fridman's podcasts](https://www.youtube.com/playlist?list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4) \n",
    "transcriptions for Question-Answering using [Andrej Karathy's](https://twitter.com/karpathy) transcriptions produced with [OpenAI's whisper](https://github.com/openai/whisper/blob/main/model-card.md) üëâÔ∏è i.e.: [Lexicap](https://karpathy.ai/lexicap/).\n",
    "\n",
    "At the moment this code relies on a private package from MeliorAI, namely [distributed-faiss](https://github.com/MeliorAI/distributed-faiss) for distributed indexing using [FAISS](https://github.com/facebookresearch/faiss) as the underlying index. Every other package is openly available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e35ef4ca-b2df-412e-911d-64a0ae90ad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq \\\n",
    "    distributed_faiss \\\n",
    "    nltk \\\n",
    "    tabulate \\\n",
    "    sentence_transformers \\\n",
    "    webvtt-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c495e775-69a7-4d2c-9db8-5d9f27c35d5f",
   "metadata": {},
   "source": [
    "## üìöÔ∏è Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e52d19e9-c55c-4b3b-9672-08e883e3aa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "            \n",
    "\n",
    "DATA_DIR = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2af8288-9cc6-46fc-87ba-3daf408bd75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import webvtt\n",
    "\n",
    "from tabulate import tabulate\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def gather_transcripts(data_dir:str = DATA_DIR, load_small:bool = False):\n",
    "    mask = \"*_small.vtt\" if load_small else \"*_large.vtt\"\n",
    "    return sorted(\n",
    "        glob.glob(os.path.join(data_dir, \"vtt\", mask))\n",
    "    )\n",
    "\n",
    "\n",
    "def gather_episode_data(data_dir:str = DATA_DIR):\n",
    "    ep_data = {}\n",
    "    with open(os.path.join(data_dir, \"episode_names.txt\")) as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            ep_num = re.findall(\"(?<=#)\\d+\", line)[0]\n",
    "            line = re.sub(f\"^{ep_num} \", \"\", line)\n",
    "            guest_and_title = line.split(\" | \")[0]\n",
    "            ep_data[ep_num] = {\n",
    "                \"guest\": guest_and_title.split(\": \")[0], \n",
    "                \"title\": \": \".join(guest_and_title.split(\": \")[1:])\n",
    "            }\n",
    "    \n",
    "    return ep_data\n",
    "\n",
    "    \n",
    "def sent_split(text:str):\n",
    "    \"\"\"Divides each document section into several chunks based on\n",
    "    a NLTK **sentence** tokenizer.\n",
    "    \"\"\"\n",
    "    text_chunks = nltk.tokenize.sent_tokenize(text)\n",
    "    text_indices = list(map(\n",
    "        lambda m: (m.start(), m.end()) if m else (None, None),\n",
    "        [re.search(re.escape(text_chunk), text) for text_chunk in text_chunks]\n",
    "    ))\n",
    "    \n",
    "    return list(zip(text_chunks, text_indices))\n",
    "\n",
    "\n",
    "def print_datapoint(datapoint, lim:int = -1):\n",
    "    keys = [\"ep_num\", \"guest\", \"title\"]\n",
    "    print(tabulate([[datapoint[k] for k in keys]], \n",
    "                   headers=keys,\n",
    "                  tablefmt=\"grid\"))\n",
    "    print(tabulate(map(lambda x: (x[0], str(x[1])), datapoint[\"texts\"][:lim]), \n",
    "                   headers=[\"text\", \"indices\"],\n",
    "                   tablefmt=\"grid\",\n",
    "                   maxcolwidths=55))\n",
    "    \n",
    "    \n",
    "def build_dataset():\n",
    "    script_files = gather_transcripts()\n",
    "    ep_data = gather_episode_data()\n",
    "    print(f\"Total episodes transcripts: {len(script_files)}\")\n",
    "\n",
    "    # Compile a dataset of transcripts with its episode information\n",
    "    dataset = []\n",
    "    data_iter = tqdm(script_files)\n",
    "    sent_id = 0\n",
    "    for sfile in data_iter:\n",
    "        fname = os.path.basename(sfile)\n",
    "        data_iter.set_description(fname)\n",
    "\n",
    "        # Episode number\n",
    "        ep_num = str(int(re.findall(\"\\d{1,3}\", fname)[0]))  # trim leading 0's\n",
    "        ep_info = ep_data[ep_num]\n",
    "\n",
    "        # Read transcript and split in sentences\n",
    "        text = \" \".join([caption.text for caption in webvtt.read(sfile)])\n",
    "        text_chunks = sent_split(text)\n",
    "        nt = len(text_chunks)\n",
    "\n",
    "        ep_info[\"text_ids\"] = list(range(sent_id, sent_id + nt))\n",
    "        ep_info[\"texts\"] = text_chunks\n",
    "        ep_info[\"ep_num\"] = ep_num\n",
    "        \n",
    "        dataset.append(ep_info)\n",
    "        sent_id += nt\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ad73605-e4eb-4e9f-9293-97450bdc3b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total episodes transcripts: 113\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d135313d9cce4381bc4f8331e0d89b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+\n",
      "|   ep_num | guest            | title   |\n",
      "+==========+==================+=========+\n",
      "|        6 | Guido van Rossum | Python  |\n",
      "+----------+------------------+---------+\n",
      "+---------------------------------------------------------+--------------+\n",
      "| text                                                    | indices      |\n",
      "+=========================================================+==============+\n",
      "| The following is a conversation with Guido van Rossum,  | (0, 353)     |\n",
      "| creator of Python, one of the most popular  programming |              |\n",
      "| languages in the world, used in almost any application  |              |\n",
      "| that involves computers  from web back end development  |              |\n",
      "| to psychology, neuroscience, computer vision, robotics, |              |\n",
      "| deep  learning, natural language processing, and almost |              |\n",
      "| any subfield of AI.                                     |              |\n",
      "+---------------------------------------------------------+--------------+\n",
      "| This conversation is part of  MIT course on artificial  | (354, 470)   |\n",
      "| general intelligence and the artificial intelligence    |              |\n",
      "| podcast.                                                |              |\n",
      "+---------------------------------------------------------+--------------+\n",
      "| If you enjoy it, subscribe on YouTube, iTunes, or your  | (472, 682)   |\n",
      "| podcast provider of choice, or simply connect  with me  |              |\n",
      "| on Twitter at Lex Friedman, spelled F R I D. And now,   |              |\n",
      "| here's my conversation with Guido van  Rossum.          |              |\n",
      "+---------------------------------------------------------+--------------+\n",
      "| You were born in the Netherlands in 1956.               | (683, 724)   |\n",
      "+---------------------------------------------------------+--------------+\n",
      "| Your parents and the world around you was deeply        | (725, 848)   |\n",
      "| deeply impacted by World War Two, as was my family from |              |\n",
      "| the Soviet Union.                                       |              |\n",
      "+---------------------------------------------------------+--------------+\n",
      "| So with that context,  what is your view of human       | (849, 906)   |\n",
      "| nature?                                                 |              |\n",
      "+---------------------------------------------------------+--------------+\n",
      "| Are some humans inherently good,  and some inherently   | (907, 966)   |\n",
      "| evil?                                                   |              |\n",
      "+---------------------------------------------------------+--------------+\n",
      "| Or do we all have both good and evil within us?         | (967, 1014)  |\n",
      "+---------------------------------------------------------+--------------+\n",
      "| Guido van Rossum Ouch, I did not expect such a deep     | (1016, 1072) |\n",
      "| one.                                                    |              |\n",
      "+---------------------------------------------------------+--------------+\n",
      "| I, I guess we all have good and evil  potential in us.  | (1073, 1127) |\n",
      "+---------------------------------------------------------+--------------+\n",
      "| And a lot of it depends on circumstances and context.   | (1128, 1181) |\n",
      "+---------------------------------------------------------+--------------+\n",
      "| Peter Bell out of that world, at least on the Soviet    | (1183, 1396) |\n",
      "| Union side in Europe, sort of out of  suffering, out of |              |\n",
      "| challenge, out of that kind of set of traumatic events, |              |\n",
      "| often emerges beautiful  art, music, literature.        |              |\n",
      "+---------------------------------------------------------+--------------+\n",
      "| In an interview I read or heard, you said you enjoyed   | (1397, 1491) |\n",
      "| Dutch literature  when you were a child.                |              |\n",
      "+---------------------------------------------------------+--------------+\n",
      "| Can you tell me about the books that had an influence   | (1492, 1572) |\n",
      "| on you in your  childhood?                              |              |\n",
      "+---------------------------------------------------------+--------------+\n",
      "| Guido van Rossum  Well, with as a teenager, my favorite | (1573, 1825) |\n",
      "| writer was my favorite Dutch author was a guy named     |              |\n",
      "| Willem  Frederik Hermans, who's writing, certainly his  |              |\n",
      "| early novels were all about sort of  ambiguous things   |              |\n",
      "| that happened during World War Two.                     |              |\n",
      "+---------------------------------------------------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "dataset = build_dataset()\n",
    "\n",
    "# print one of the elements of the dataset\n",
    "print_datapoint(dataset[1], lim=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637cbfa6-bb6c-4312-963a-d10e936157df",
   "metadata": {},
   "source": [
    "## üß™ Semantic Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90232e74-26e1-4c33-a894-a2ff1e3ddd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Can also be used from ü§ó-Transformers:\n",
    "# https://huggingface.co/sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco\n",
    "embedder = SentenceTransformer('msmarco-distilbert-base-tas-b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7db6c5c3-b639-4955-a770-5d73879708c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c034b1b66f4343b151dc24c4434b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_iter = tqdm(dataset)\n",
    "\n",
    "for dpoint in data_iter:\n",
    "    data_iter.set_description(f\"{dpoint['ep_num']}: {dpoint['title']}\")\n",
    "    dpoint[\"embeddings\"] = embedder.encode(dpoint['texts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3fc44ee-2a68-4043-8572-af4ce3b730b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(737, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"embeddings\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077fd007-9438-47e0-8b0d-f25bd5fdbf56",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üì¶Ô∏è Indexing: Distributed FAISS\n",
    "\n",
    "First we need to run the FAISS server (in a separate terminal):\n",
    "\n",
    "```python\n",
    "python dfaiss_server.py \\\n",
    "    --log-dir ./logs \\\n",
    "    --partition 1 \\\n",
    "    --discovery-config dfaiss.discovery \\\n",
    "    --num-servers 1 \\\n",
    "    --num-servers-per-node 1 \\\n",
    "    --mem-gb 4 \\\n",
    "    --load-index\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bef92811-60c2-47ae-bee7-10dfb320f606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "from distributed_faiss.client import IndexClient\n",
    "from distributed_faiss.index_cfg import IndexCfg\n",
    "\n",
    "\n",
    "def _validate_metric(self):\n",
    "    if self.metric not in VALID_METRICS:\n",
    "        logger.error(\n",
    "            f\"{self.metric} is not a valid Metric. \"\n",
    "            f\"Try to choose between {VALID_METRICS}\"\n",
    "        )\n",
    "        raise ValueError(\n",
    "            f\"{self.metric} is not a valid Metric. \"\n",
    "            f\"Try to choose between {VALID_METRICS}\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "def init_client(index_id:str, index_cfg_file:str, discovery_file:str):\n",
    "    with open(index_cfg_file, \"r\") as f:\n",
    "        cfg = json.load(f)\n",
    "        idx_cfg = IndexCfg(**cfg)\n",
    "\n",
    "    index_client = IndexClient(discovery_file)\n",
    "\n",
    "    if not index_client.index_loaded(index_id):\n",
    "        idx_loaded = index_client.load_index(index_id, idx_cfg)\n",
    "        if idx_loaded is False:\n",
    "            print(f\"Index {index_id} hasn't been loaded. Creating new index...\")\n",
    "            index_client.create_index(index_id, idx_cfg)\n",
    "            \n",
    "    return index_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7600bf7-b522-4054-9054-650ce241d8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_INDEX_TYPES = [\"Flat\", \"IVF\"]\n",
    "VALID_METRICS = [\n",
    "    \"l2\",\n",
    "    \"dot\",\n",
    "]\n",
    "\n",
    "# configuration and discovery files\n",
    "discovery_file = \"./dfaiss.discovery\"\n",
    "index_cfg_file = \"./idx_cfg.json\"\n",
    "index_id = \"local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5792299-0d07-4573-8da0-bc8ec7c76caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting jose-N501VW 12032 AddressFamily.AF_INET\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca1427e20c544d8b16f932bfe1b4032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the dfaiss client\n",
    "index_client = init_client(index_id, index_cfg_file, discovery_file)\n",
    "\n",
    "data_iter = tqdm(dataset)\n",
    "\n",
    "for dpoint in data_iter:\n",
    "    data_iter.set_description(f\"{dpoint['ep_num']}: {dpoint['title']}\")\n",
    "    embeddings = dpoint[\"embeddings\"]\n",
    "    n_vec = embeddings.shape[0]\n",
    "    meta = {k: dpoint[k] for k in [\"ep_num\", \"guest\", \"title\"]}\n",
    "    index_client.add_index_data(\n",
    "        index_id, \n",
    "        embeddings, \n",
    "        [{\"id\": tid, **meta} for tid in dpoint[\"text_ids\"]]\n",
    "    )\n",
    "    \n",
    "index_client.sync_train(index_id)\n",
    "index_client.add_buffer_to_index(index_id)\n",
    "index_client.save_index(index_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "11c8e720-ecb1-4ddc-924f-0c5b0e2faf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting jose-N501VW 12032 AddressFamily.AF_INET\n",
      "connecting jose-N501VW 12033 AddressFamily.AF_INET\n"
     ]
    }
   ],
   "source": [
    "index_client = init_client(index_id, index_cfg_file, discovery_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af04c6f1-b2a5-4484-8a0a-a75e2979bdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num servers: 1\n",
      "Server 1: IndexState.ADD\n"
     ]
    }
   ],
   "source": [
    "print(f\"num servers: {index_client.get_num_servers()}\")\n",
    "for sn, state in enumerate(index_client.get_all_states(index_id)):\n",
    "    print(f\"Server {sn+1}: {state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7da2688-6195-4b08-b101-eb64a691c7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76984"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_client.get_ntotal(index_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5714f8a2-9de7-4b8c-a2d3-c4d4893e88e0",
   "metadata": {},
   "source": [
    "## üîçÔ∏è QA Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7c7ec8a-a285-460f-b4c0-bbb619cc3e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index loaded: True\n",
      "[[{'id': 96511}, {'id': 25577}, {'id': 96600}, {'id': 56700}, {'id': 64434}, {'id': 0}, {'id': 5391}, {'id': 1808}, {'id': 149383}, {'id': 31468}]]\n",
      "[array([-99.35968 , -98.620026, -97.52474 , -97.056656, -96.73923 ,\n",
      "       -96.72657 , -95.80153 , -95.267555, -93.477715, -93.45641 ],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "index_loaded = any(index_client.all_index_loaded(index_id))\n",
    "if not index_loaded:\n",
    "    index_client.load_index(index_id)\n",
    "    \n",
    "index_loaded = any(index_client.all_index_loaded(index_id))\n",
    "print(f\"Index loaded: {index_loaded}\")\n",
    "\n",
    "scores, indices = index_client.search(\n",
    "    embedder.encode([\"Which boooks where written by Max Tegmark?\"]),\n",
    "    top_k=10,\n",
    "    index_id=index_id,\n",
    "    return_embeddings=False,\n",
    "    filter_dict={},\n",
    ")\n",
    "print(indices)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a39f92ea-a7ab-4dd9-b144-8d9150bac91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_to_dp_idx = {\n",
    "    dpoint[\"ep_num\"]: i \n",
    "    for i, dpoint in enumerate(dataset)\n",
    "}\n",
    "\n",
    "tid_to_ep = {\n",
    "    tid: dpoint[\"ep_num\"]\n",
    "    for dpoint in dataset\n",
    "    for tid in dpoint[\"text_ids\"]\n",
    "}\n",
    "\n",
    "\n",
    "def tid_to_episode(tid:int):\n",
    "    return dataset[ep_to_dp_idx[tid_to_ep[tid]]]\n",
    "\n",
    "\n",
    "def tid_to_text(tid:int):\n",
    "    epi = tid_to_episode(tid)\n",
    "    idx = epi[\"text_ids\"].index(tid)\n",
    "    return epi[\"texts\"][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b402d042-5b15-455d-86e9-162411833566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96511 --> ('And I guess I follow Max Tegmark here.', (154099, 154137))\n",
      "25577 --> (\"And by the way, I'm borrowing from Max Tegmark  for some of these metaphors, the physicist.\", (93069, 93160))\n",
      "96600 --> (\"I mean, talking about people like Lee Small  and Alan Guth, Max Tegmark, okay, we're really smart.\", (163510, 163608))\n",
      "56700 --> (\"Tegmark, I view as a philosopher  who is somehow taking credit for Platonism,  which I don't see any reason for fighting with Max  because I like Max, but if it ever comes time,  I'm putting a post it note that I'm not positive  the mathematical universe hypothesis  is really anything new.\", (125870, 126160))\n",
      "64434 --> ('Like you have like the Max Tegmark,  young version of Max Tegmark,  who knows how to play the role of boring and fitting in.', (80770, 80894))\n",
      "0 --> (\" As part of MIT course 6S099, Artificial General Intelligence,  I've gotten the chance to sit down with Max Tegmark.\", (0, 116))\n",
      "5391 --> ('Actually, a lot of physicists, Max Tegmark,  people who think the universe  is an information processing system,  our brain is kind of an information processing system.', (3340, 3508))\n",
      "1808 --> ('So I talked to Max Tegmark and Stuart Russell,  who are very concerned about existential threats of AI.', (51379, 51482))\n",
      "149383 --> ('And then Allen Ginsberg wrote a poem in the 60s,  which incredible poem called Howl about this thing Moloch.', (75828, 75936))\n",
      "31468 --> (\"Well, he'd be on like TikTok and Instagram  and he would never write the great works he's written.\", (27435, 27533))\n"
     ]
    }
   ],
   "source": [
    "for ind in indices[0]:\n",
    "    tid = ind[\"id\"]\n",
    "    print(f\"{tid} --> {tid_to_text(tid)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
