{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1c34396-f1eb-4977-87ed-a70ff8e61d11",
   "metadata": {},
   "source": [
    "# Lexicap\n",
    "\n",
    "This notebook aims to index [Lex Fridman's podcasts](https://www.youtube.com/playlist?list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4) \n",
    "transcriptions for Question-Answering using [Andrej Karathy's](https://twitter.com/karpathy) transcriptions produced with [OpenAI's whisper](https://github.com/openai/whisper/blob/main/model-card.md) üëâÔ∏è [Lexicap](https://karpathy.ai/lexicap/).\n",
    "\n",
    "At the moment this code relies on a private package from MeliorAI, namely [distributed-faiss](https://github.com/MeliorAI/distributed-faiss) for distributed indexing using [FAISS](https://github.com/facebookresearch/faiss) as the underlying index. Every other package is openly available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e35ef4ca-b2df-412e-911d-64a0ae90ad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq \\\n",
    "    distributed_faiss \\\n",
    "    nltk \\\n",
    "    tabulate \\\n",
    "    sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c495e775-69a7-4d2c-9db8-5d9f27c35d5f",
   "metadata": {},
   "source": [
    "## üìöÔ∏è Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e52d19e9-c55c-4b3b-9672-08e883e3aa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "            \n",
    "\n",
    "DATA_DIR = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b2af8288-9cc6-46fc-87ba-3daf408bd75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import webvtt\n",
    "\n",
    "from tabulate import tabulate\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def gather_transcripts(data_dir:str = DATA_DIR, load_small:bool = False):\n",
    "    mask = \"*_small.vtt\" if load_small else \"*_large.vtt\"\n",
    "    return sorted(\n",
    "        glob.glob(os.path.join(data_dir, \"vtt\", mask))\n",
    "    )\n",
    "\n",
    "\n",
    "def gather_episode_data(data_dir:str = DATA_DIR):\n",
    "    ep_data = {}\n",
    "    with open(os.path.join(data_dir, \"episode_names.txt\")) as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            ep_num = re.findall(\"(?<=#)\\d+\", line)[0]\n",
    "            line = re.sub(f\"^{ep_num} \", \"\", line)\n",
    "            guest_and_title = line.split(\" | \")[0]\n",
    "            ep_data[ep_num] = {\n",
    "                \"guest\": guest_and_title.split(\": \")[0], \n",
    "                \"title\": \": \".join(guest_and_title.split(\": \")[1:])\n",
    "            }\n",
    "    \n",
    "    return ep_data\n",
    "\n",
    "    \n",
    "def sent_split(text:str):\n",
    "    \"\"\"Divides each document section into several chunks based on\n",
    "    a NLTK **sentence** tokenizer.\n",
    "    \"\"\"\n",
    "    text_chunks = nltk.tokenize.sent_tokenize(text)\n",
    "    text_indices = list(map(\n",
    "        lambda m: (m.start(), m.end()) if m else (None, None),\n",
    "        [re.search(re.escape(text_chunk), text) for text_chunk in text_chunks]\n",
    "    ))\n",
    "    \n",
    "    return list(zip(text_chunks, text_indices))\n",
    "\n",
    "\n",
    "def print_datapoint(datapoint, lim:int = -1):\n",
    "    keys = [\"ep_num\", \"guest\", \"title\"]\n",
    "    print(tabulate([[datapoint[k] for k in keys]], \n",
    "                   headers=keys,\n",
    "                  tablefmt=\"grid\"))\n",
    "    print(tabulate(map(lambda x: (x[0], str(x[1])), datapoint[\"texts\"][:lim]), \n",
    "                   headers=[\"text\", \"indices\"],\n",
    "                   tablefmt=\"grid\",\n",
    "                   maxcolwidths=55))\n",
    "    \n",
    "    \n",
    "def build_dataset():\n",
    "    script_files = gather_transcripts()\n",
    "    ep_data = gather_episode_data()\n",
    "    print(f\"Total episodes transcripts: {len(script_files)}\")\n",
    "\n",
    "    # Compile a dataset of transcripts with its episode information\n",
    "    dataset = []\n",
    "    data_iter = tqdm(script_files)\n",
    "    sent_id = 0\n",
    "    for sfile in data_iter:\n",
    "        fname = os.path.basename(sfile)\n",
    "        data_iter.set_description(fname)\n",
    "\n",
    "        # Episode number\n",
    "        ep_num = str(int(re.findall(\"\\d{1,3}\", fname)[0]))  # trim leading 0's\n",
    "        ep_info = ep_data[ep_num]\n",
    "\n",
    "        # Read transcript and split in sentences\n",
    "        text = \" \".join([caption.text for caption in webvtt.read(sfile)])\n",
    "        text_chunks = sent_split(text)\n",
    "        nt = len(text_chunks)\n",
    "\n",
    "        ep_info[\"text_ids\"] = list(range(sent_id, sent_id + nt))\n",
    "        ep_info[\"texts\"] = text_chunks\n",
    "        ep_info[\"ep_num\"] = ep_num\n",
    "        \n",
    "        dataset.append(ep_info)\n",
    "        sent_id += nt\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "1ad73605-e4eb-4e9f-9293-97450bdc3b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total episodes transcripts: 113\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cbbef15b7564cf3a6849de7775cfdbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+\n",
      "|   ep_num | guest            | title   |\n",
      "+==========+==================+=========+\n",
      "|        6 | Guido van Rossum | Python  |\n",
      "+----------+------------------+---------+\n",
      "+---------------------------------------------------------+--------------+\n",
      "| text                                                    | indices      |\n",
      "+=========================================================+==============+\n",
      "| The following is a conversation with Guido van Rossum,  | (0, 353)     |\n",
      "| creator of Python, one of the most popular  programming |              |\n",
      "| languages in the world, used in almost any application  |              |\n",
      "| that involves computers  from web back end development  |              |\n",
      "| to psychology, neuroscience, computer vision, robotics, |              |\n",
      "| deep  learning, natural language processing, and almost |              |\n",
      "| any subfield of AI.                                     |              |\n",
      "+---------------------------------------------------------+--------------+\n",
      "| This conversation is part of  MIT course on artificial  | (354, 470)   |\n",
      "| general intelligence and the artificial intelligence    |              |\n",
      "| podcast.                                                |              |\n",
      "+---------------------------------------------------------+--------------+\n",
      "| If you enjoy it, subscribe on YouTube, iTunes, or your  | (472, 682)   |\n",
      "| podcast provider of choice, or simply connect  with me  |              |\n",
      "| on Twitter at Lex Friedman, spelled F R I D. And now,   |              |\n",
      "| here's my conversation with Guido van  Rossum.          |              |\n",
      "+---------------------------------------------------------+--------------+\n",
      "| You were born in the Netherlands in 1956.               | (683, 724)   |\n",
      "+---------------------------------------------------------+--------------+\n",
      "| Your parents and the world around you was deeply        | (725, 848)   |\n",
      "| deeply impacted by World War Two, as was my family from |              |\n",
      "| the Soviet Union.                                       |              |\n",
      "+---------------------------------------------------------+--------------+\n",
      "| So with that context,  what is your view of human       | (849, 906)   |\n",
      "| nature?                                                 |              |\n",
      "+---------------------------------------------------------+--------------+\n",
      "| Are some humans inherently good,  and some inherently   | (907, 966)   |\n",
      "| evil?                                                   |              |\n",
      "+---------------------------------------------------------+--------------+\n",
      "| Or do we all have both good and evil within us?         | (967, 1014)  |\n",
      "+---------------------------------------------------------+--------------+\n",
      "| Guido van Rossum Ouch, I did not expect such a deep     | (1016, 1072) |\n",
      "| one.                                                    |              |\n",
      "+---------------------------------------------------------+--------------+\n",
      "| I, I guess we all have good and evil  potential in us.  | (1073, 1127) |\n",
      "+---------------------------------------------------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "dataset = build_dataset()\n",
    "\n",
    "# print one of the elements of the dataset\n",
    "print_datapoint(dataset[1], lim=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637cbfa6-bb6c-4312-963a-d10e936157df",
   "metadata": {},
   "source": [
    "## üß™ Semantic Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "90232e74-26e1-4c33-a894-a2ff1e3ddd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Can also be used from ü§ó-Transformers:\n",
    "# https://huggingface.co/sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco\n",
    "embedder = SentenceTransformer('msmarco-distilbert-base-tas-b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db6c5c3-b639-4955-a770-5d73879708c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8710970520674497aebd586eba401333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_iter = tqdm(dataset)\n",
    "\n",
    "for dpoint in data_iter:\n",
    "    data_iter.set_description(f\"{dpoint['ep_num']}: {dpoint['title']}\")\n",
    "    dpoint[\"embeddings\"] = embedder.encode(dpoint['texts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fc44ee-2a68-4043-8572-af4ce3b730b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][\"embeddings\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077fd007-9438-47e0-8b0d-f25bd5fdbf56",
   "metadata": {},
   "source": [
    "## üì¶Ô∏è Indexing: Distributed FAISS\n",
    "\n",
    "First we need to run the FAISS server:\n",
    "\n",
    "```python\n",
    "python dfaiss_server.py \\\n",
    "    --log-dir ./logs \\\n",
    "    --partition 1 \\\n",
    "    --discovery-config dfaiss.discovery \\\n",
    "    --num-servers 1 \\\n",
    "    --num-servers-per-node 1 \\\n",
    "    --mem-gb 4 \\\n",
    "    --load-index\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bef92811-60c2-47ae-bee7-10dfb320f606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "from distributed_faiss.client import IndexClient\n",
    "from distributed_faiss.index_cfg import IndexCfg\n",
    "\n",
    "\n",
    "def _validate_metric(self):\n",
    "    if self.metric not in VALID_METRICS:\n",
    "        logger.error(\n",
    "            f\"{self.metric} is not a valid Metric. \"\n",
    "            f\"Try to choose between {VALID_METRICS}\"\n",
    "        )\n",
    "        raise ValueError(\n",
    "            f\"{self.metric} is not a valid Metric. \"\n",
    "            f\"Try to choose between {VALID_METRICS}\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "def init_client(index_id:str, index_cfg_file:str, discovery_file:str):\n",
    "    with open(index_cfg_file, \"r\") as f:\n",
    "        cfg = json.load(f)\n",
    "        idx_cfg = IndexCfg(**cfg)\n",
    "\n",
    "    index_client = IndexClient(discovery_file)\n",
    "\n",
    "    if not index_client.index_loaded(index_id):\n",
    "        idx_loaded = index_client.load_index(index_id, idx_cfg)\n",
    "        if idx_loaded is False:\n",
    "            print(f\"Index {index_id} hasn't been loaded. Creating new index...\")\n",
    "            index_client.create_index(index_id, idx_cfg)\n",
    "            \n",
    "    return index_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f7600bf7-b522-4054-9054-650ce241d8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_INDEX_TYPES = [\"Flat\", \"IVF\"]\n",
    "VALID_METRICS = [\n",
    "    \"l2\",\n",
    "    \"dot\",\n",
    "]\n",
    "\n",
    "# configuration and discovery files\n",
    "discovery_file = \"./dfaiss.discovery\"\n",
    "index_cfg_file = \"./idx_cfg.json\"\n",
    "index_id = \"local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d5792299-0d07-4573-8da0-bc8ec7c76caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting jose-N501VW 12032 AddressFamily.AF_INET\n",
      "connecting jose-N501VW 12033 AddressFamily.AF_INET\n",
      "Index local hasn't been loaded. Creating new index...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a043ee96c5164ffe87a6fdacd018238e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the dfaiss client\n",
    "index_client = init_client(index_id, index_cfg_file, discovery_file)\n",
    "\n",
    "data_iter = tqdm(dataset)\n",
    "\n",
    "\n",
    "for dpoint in data_iter:\n",
    "    data_iter.set_description(f\"{dpoint['ep_num']}: {dpoint['title']}\")\n",
    "    embeddings = dpoint[\"embeddings\"]\n",
    "    n_vec = embeddings.shape[0]\n",
    "    meta = {dpoint[k] for k in [\"ep_num\", \"guest\", \"title\"]}\n",
    "    index_client.add_index_data(\n",
    "        index_id, \n",
    "        embeddings, \n",
    "        [{\"id\": tid, **meta} for tid in dpoint[\"text_ids\"]]\n",
    "    )\n",
    "    chunk_id += n_vec\n",
    "    \n",
    "index_client.sync_train(index_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "af04c6f1-b2a5-4484-8a0a-a75e2979bdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num servers: 2\n",
      "Index states: [<IndexState.TRAINED: 4>, <IndexState.TRAINED: 4>]\n"
     ]
    }
   ],
   "source": [
    "index_client.add_buffer_to_index(index_id)\n",
    "index_client.save_index(index_id)\n",
    "\n",
    "print(f\"num servers: {index_client.get_num_servers()}\")\n",
    "print(f\"Index states: {index_client.get_all_states(index_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b7da2688-6195-4b08-b101-eb64a691c7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153933"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_client.get_ntotal(index_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5714f8a2-9de7-4b8c-a2d3-c4d4893e88e0",
   "metadata": {},
   "source": [
    "## üîçÔ∏è QA Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c7c7ec8a-a285-460f-b4c0-bbb619cc3e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index loaded: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([-109.61733 , -106.72429 , -106.32414 , -105.41661 , -104.84072 ,\n",
       "         -104.7509  , -104.35072 , -104.229034, -103.04799 , -102.74254 ],\n",
       "        dtype=float32)],\n",
       " [[{'id': 774},\n",
       "   {'id': 782},\n",
       "   {'id': 766},\n",
       "   {'id': 770},\n",
       "   {'id': 759},\n",
       "   {'id': 2339},\n",
       "   {'id': 763},\n",
       "   {'id': 755},\n",
       "   {'id': 751},\n",
       "   {'id': 745}]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_loaded = any(index_client.all_index_loaded(index_id))\n",
    "print(f\"Index loaded: {index_loaded}\")\n",
    "\n",
    "index_client.search(\n",
    "    embedder.encode([\"Who's Guido van Rossum?\"]),\n",
    "    top_k=10,\n",
    "    index_id=index_id,\n",
    "    return_embeddings=False,\n",
    "    filter_dict={},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
